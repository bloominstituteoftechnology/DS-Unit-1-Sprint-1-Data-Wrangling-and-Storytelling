{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_113_Join_and_Reshape_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhender8/DS-Unit-1-Sprint-1-Data-Wrangling-and-Storytelling/blob/master/LS_DS_113_Join_and_Reshape_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQqT5ie4vdDS"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 1, Sprint 1, Module 3*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOI3YimSvopw"
      },
      "source": [
        "# Join and Reshape Data \n",
        "\n",
        "- Objective 01 - concatenate data using the pandas concat method\n",
        "- Objective 02 - merge data using pandas merge\n",
        "- Objective 03 - define the concept of tidy data and describe the format\n",
        "- Objective 04 - transition between tidy and wide data formats with `melt()` and `pivot()`\n",
        "\n",
        "Helpful Links:\n",
        "- [Pandas Cheat Sheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf)\n",
        "- [Tidy Data](https://en.wikipedia.org/wiki/Tidy_data)\n",
        "  - Combine Data Sets: Standard Joins\n",
        "  - Tidy Data\n",
        "  - Reshaping Data\n",
        "- Python Data Science Handbook\n",
        "  - [Chapter 3.6](https://jakevdp.github.io/PythonDataScienceHandbook/03.06-concat-and-append.html), Combining Datasets: Concat and Append\n",
        "  - [Chapter 3.7](https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html), Combining Datasets: Merge and Join\n",
        "  - [Chapter 3.8](https://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html), Aggregation and Grouping\n",
        "  - [Chapter 3.9](https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html), Pivot Tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_gXHprXvqVx"
      },
      "source": [
        "# [Objective 1](#concat) - Concatenate dataframes with pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPVHZevR04pV"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\"Concatenate\" is a fancy word for joining two things together. For example, we can concatenate two strings together using the `+` operator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeAeYKwN08q3",
        "outputId": "2a04e5b7-4d78-4a79-b4a4-d8adbad71121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'We can join/concatenate two strings together ' + 'using the \"+\" operator.'\n",
        "#this is a test comment to save\n",
        "#this is a second test comment to save again"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We can join/concatenate two strings together using the \"+\" operator.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIgxXomn7iHC"
      },
      "source": [
        "When we \"concatenate\" two dataframes we will \"stick them together\" either by rows or columns. Lets look at some simple examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6MbummV9kgH"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1aKHYuH8BTX"
      },
      "source": [
        "df1 = pd.DataFrame({'a': [1,2,3,4], 'b': [4,5,6,7], 'c': [7,8,9,10]})\n",
        "\n",
        "df2 = pd.DataFrame({'a': [6,4,8,7], 'b': [9,4,3,2], 'c': [1,6,2,9]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blLFOpK-8Zwq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olRWT5VK8bl2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBh-mGzI8k3l"
      },
      "source": [
        "### Concatenate by Rows \n",
        "\n",
        "concatenating by rows is the default behavior of `pd.concat()` This is often the most common form of concatenation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCw6DJxR8m6T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iFaja6QacVx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvZH9k-e8ohe"
      },
      "source": [
        "### Concatenate by Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fCzFQxx9D7b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggv1k1DlbATJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUsQ7O9RbMvu"
      },
      "source": [
        "# rename columns\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TCPw9lnbTrY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-NxpMCr9WOS"
      },
      "source": [
        "When concatenating dataframes, it is done using the column headers and row index values to match rows up. If these don't match up, then `NaN` values will be added where matches can't be found. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luR-nvD99tBa"
      },
      "source": [
        "df3 = pd.DataFrame({'a': [4,3,2,1], 'b': [4,5,6,7], 'c': [7,8,9,10]})\n",
        "\n",
        "df4 = pd.DataFrame({'a': [6,4,8,7,8], 'b': [9,4,3,2,1], 'd': [1,6,2,9,5]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj-FdzVf97mn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfZ_wekl99-e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOuoIdey-kCD"
      },
      "source": [
        "### Concatenate by rows when not all column headers match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FpZdgat-EQD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al203GNp-qVS"
      },
      "source": [
        "### Concatenate by columns when not all row indexes match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc2ngk3O-YCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUpWwpdSBJGd"
      },
      "source": [
        "Whenever we are combining dataframes, if appropriate values cannot be found based on the rules of the method we are using, then missing values will be filled with `NaNs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8YGJ8Wm1AG3"
      },
      "source": [
        "## Follow Along\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbH8CcozBcyI"
      },
      "source": [
        "We’ll work with a dataset of [3 Million Instacart Orders, Open Sourced](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)!\n",
        "\n",
        "The files that we will be working with are in a folder of CSVs, we need to load that folder of CSVs, explore the CSVs to make sure that we understand what we're working with, and where the important data lies, and then work to combine the dataframes together as necessary. \n",
        "\n",
        "\n",
        "\n",
        "Our goal is to reproduce this table which holds the first two orders for user id 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xugHGV5C60D"
      },
      "source": [
        "from IPython.display import display, Image\n",
        "url = 'https://cdn-images-1.medium.com/max/1600/1*vYGFQCafJtGBBX5mbl0xyw.png'\n",
        "example = Image(url=url, width=600)\n",
        "\n",
        "display(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E3wKrdTChuC"
      },
      "source": [
        "#!wget https://s3.amazonaws.com/instacart-datasets/instacart_online_grocery_shopping_2017_05_01.tar.gz \n",
        "\n",
        "# Make sure we're in the top-level /content directory\n",
        "#\n",
        "# See below for notes on the cd command and why it's %cd instead of !cd\n",
        "%cd /content\n",
        "\n",
        "# Remove everything in the current working directory\n",
        "#\n",
        "# rm is the remove command\n",
        "# -rf specifies the \"recursive\" and \"force\" options to remove all files in \n",
        "# subdirectories without prompting\n",
        "#\n",
        "# THIS IS A POWERFUL COMMAND! (NEVER RUN THIS COMMAND ON YOUR COMPUTER)\n",
        "#\n",
        "# In this particular case, removing all of the files makes things easier if you\n",
        "# need to re-run these examples by allowing you start with a clean directory\n",
        "# every time.\n",
        "!rm -rf *\n",
        "\n",
        "# wget retrieves files from a remote location\n",
        "!wget https://www.dropbox.com/s/pofcl26lvoj6073/instacart-market-basket-analysis.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkxqMNGrDJrM"
      },
      "source": [
        "# Unzip the archive\n",
        "#\n",
        "# Creates a new directory called instacart-market-basket-analysis\n",
        "\n",
        "!unzip instacart-market-basket-analysis.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaDdXbQqCnGc"
      },
      "source": [
        "# Change into the newly-unzipped directory\n",
        "#\n",
        "# % sign is required to change to a new directory -- you can't use !cd like\n",
        "# other commands\n",
        "#\n",
        "# Optional technical details:\n",
        "#\n",
        "# % makes the command apply to the **entire notebook environment**, which is\n",
        "# what you need to do to change the working directory\n",
        "#\n",
        "# The ! sign **opens a new shell process** behind the scenes to execute the\n",
        "# command -- this works fine for regular commands like unzip and ls\n",
        "#\n",
        "# Therefore, !cd would apply only to that new shell and wouldn't change the\n",
        "# global notebook environment\n",
        "#\n",
        "# If this makes your heard hurt, don't worry too much about it. We'll talk\n",
        "# more about the shell and operating systems stuff later in the program.\n",
        "\n",
        "%cd instacart-market-basket-analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eADz6civ204P"
      },
      "source": [
        "# Unzip all .csv.zip files in the directory\n",
        "!unzip \"*.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnEkWO4E2_w2"
      },
      "source": [
        "# List all csv files in the current directory\n",
        "# -l specifies the \"long\" listing format, which includes additional info on each file\n",
        "# -h specifies \"human readable\" file size units\n",
        "!ls -l -h *.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSV7d0kqfnyJ"
      },
      "source": [
        "display(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxwByNLoEG9p"
      },
      "source": [
        "### aisles\n",
        "\n",
        "We don't need anything from aisles.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLmGMr_rCoi-"
      },
      "source": [
        "aisles = pd.read_csv('aisles.csv')\n",
        "\n",
        "print(aisles.shape)\n",
        "aisles.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTPRZsLvENgJ"
      },
      "source": [
        "### departments\n",
        "\n",
        "We don't need anything from departments.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRVjREe8D6yj"
      },
      "source": [
        "departments = pd.read_csv('departments.csv')\n",
        "\n",
        "print(departments.shape)\n",
        "departments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U38aOM6nEWOe"
      },
      "source": [
        "### order_products__prior\n",
        "\n",
        "We need:\n",
        "- order id\n",
        "- proudct id\n",
        "- add to cart order\n",
        "\n",
        "Everything except for 'reordered'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CMWcWSiD8aW"
      },
      "source": [
        "order_products__prior = pd.read_csv('order_products__prior.csv')\n",
        "\n",
        "print(order_products__prior.shape)\n",
        "order_products__prior.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KRCdSl5E63N"
      },
      "source": [
        "### order_products__train\n",
        "\n",
        "We need:\n",
        "- order id\n",
        "- proudct id\n",
        "- add to cart order\n",
        "\n",
        "Everything except for 'reordered'\n",
        "\n",
        "Do you see anything similar between order_products__train and order_products__prior?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pq7lgVUD-a-"
      },
      "source": [
        "order_products__train = pd.read_csv('order_products__train.csv')\n",
        "\n",
        "print(order_products__train.shape)\n",
        "order_products__train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2cdftjYFj1k"
      },
      "source": [
        "### orders\n",
        "\n",
        "We need:\n",
        "- order id\n",
        "- user id\n",
        "- order number\n",
        "- order dow\n",
        "- order hour of day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9PyqoneEBPd"
      },
      "source": [
        "orders = pd.read_csv('orders.csv')\n",
        "\n",
        "print(orders.shape)\n",
        "orders.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXyRiuIdFmGU"
      },
      "source": [
        "### products\n",
        "\n",
        "We need:\n",
        "- product id\n",
        "- product name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J917C0NEDhG"
      },
      "source": [
        "products = pd.read_csv('products.csv')\n",
        "\n",
        "print(products.shape)\n",
        "products.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALhA76X1GkgY"
      },
      "source": [
        "## Concatenate order_products__prior and order_products__train\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU4nkwnPGz4A"
      },
      "source": [
        "order_products = pd.concat([order_products__prior, order_products__train])\n",
        "\n",
        "order_products.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrBZ0y8TG09_"
      },
      "source": [
        "print(order_products__prior.shape)\n",
        "print(order_products__train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZk8V7yxG2Qg"
      },
      "source": [
        "print(order_products.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSiHrHuj1ME-"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Concatenating dataframes means to stick two dataframes together either by rows or by columns. The default behavior of `pd.concat()` is to take the rows of one dataframe and add them to the rows of another dataframe. If we pass the argument `axis=1` then we will be adding the columns of one dataframe to the columns of another dataframe.\n",
        "\n",
        "Concatenating dataframes is most useful when the columns are the same between two dataframes or when we have matching row indices between two dataframes. \n",
        "\n",
        "Be ready to use this method to combine dataframes together during your assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17PV3bEtz449"
      },
      "source": [
        "# [Objective 2](#merge) - Merge dataframes with pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAiMlm5Q05LW"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH4J87G4LZjd"
      },
      "source": [
        "display(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1o6R29VLwwu"
      },
      "source": [
        "Before we can continue we need to understand where the data in the above table is coming from and what why specific pieces of data are held in the specific dataframes.\n",
        "\n",
        "Each of these CSVs has a specific unit of observation (row). The columns that we see included in each CSV were selected purposefully. For example, everything each row of the `orders` dataframe is a specific and unique order -telling us who made the order, and when they made it. Every row in the `products` dataframe tells us about a specific and unique product that thestore offers. And everything in the `order_products` dataframe tells us about how products are associated with specific orders -including when the product was added to the shopping cart. \n",
        "\n",
        "### The Orders Dataframe\n",
        "\n",
        "Holds information about specific orders, things like who placed the order, what \n",
        "\n",
        "- user_id\n",
        "- order_id\n",
        "- order_number\n",
        "- order_dow\n",
        "- order_hour_of_day\n",
        "\n",
        "### The Products Dataframe\n",
        "\n",
        "Holds information about individual products.\n",
        "\n",
        "- product_id\n",
        "- product_name\n",
        "\n",
        "### The Order_Products Dataframe\n",
        "\n",
        "Tells us how products are associated with specific orders since an order is a group of products.\n",
        "\n",
        "- order_id\n",
        "- product_id\n",
        "- add_to_cart_order\n",
        "\n",
        "As we look at the table that we're trying to recreate, we notice that we're not looking at specific orders or products, but at a specific **USER**. We're looking at the first two orders for a specific user and the products associated with those orders, so we'll need to combine dataframes to get all of this data together into a single table.\n",
        "\n",
        "**The key to combining all of this information is that we need values that exist in both datasets that we can use to match up rows and combine dataframes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g38DqtNj1BnI"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "We have two dataframes, so we're going to need to merge our data twice. As we approach merging datasets together we will take the following approach.\n",
        "\n",
        "1) Identify which to dataframes we would like to combine.\n",
        "\n",
        "2) Find columns that are common between both dataframes that we can use to match up information.\n",
        "\n",
        "3) Slim down both of our dataframes so that they only relevant data before we merge.\n",
        "\n",
        "4) Merge the dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bY8l5L1msOH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq1ymXsZnDQJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_X78ApndDw"
      },
      "source": [
        "## ^^^^^ DON'T DO THIS!\n",
        "\n",
        "I just merged absolutely everything\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSuIWJZinrOL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259BJMJ43Ka5"
      },
      "source": [
        "### First Merge\n",
        "\n",
        "1) Combine `orders` and `order_products`\n",
        "\n",
        "2) We will use the `order_id` column to match information between the two datasets\n",
        "\n",
        "3) Lets slim down our dataframes to only the information that we need. We do this because the merge process is complex. Why would we merge millions of rows together if we know that we're only going to need 11 rows when we're done\n",
        "\n",
        "What specific conditions could we use to slim down the `orders` dataframe?\n",
        "\n",
        "`user_id == 1` and `order_id <=2`\n",
        "\n",
        "or\n",
        "\n",
        "`order_id == 2539329` and `order_id == 2398795`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6r8i8tN1H1S"
      },
      "source": [
        "# An example of dataframe filtering\n",
        "\n",
        "# Create a condition\n",
        "\n",
        "\n",
        "# Pass that condition into the square brackets \n",
        "# that we use to access portions of a dataframe\n",
        "# only the rows where that condition evaluates to *TRUE*\n",
        "# will be retained in the dataframe\n",
        "\n",
        "# Look at the subsetted dataframe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iuvnXjnpPPt"
      },
      "source": [
        "display(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWUCmgx66Td3"
      },
      "source": [
        "# We don't necessarily have to save our condition to the variable \"condition\"\n",
        "# we can pass the condition into the square brackest directly\n",
        "# I just wanted to be clear what was happening inside of the square brackets\n",
        "\n",
        "\n",
        "# orders[0:10,0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwNuTBbwqeVQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny-eJnuMqhmj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pga6V2eeqC0m"
      },
      "source": [
        "# Filter based on user_id and order_number\n",
        "# AND condition version \n",
        "# I need to use the \"bitwise\" and operator: &\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kQG4sxP6lod"
      },
      "source": [
        "Remember there are multiple ways that we could have filtered this dataframe. We also could have done it by specific `order_id`s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IA4Kwyw6vk6"
      },
      "source": [
        "# use the bitwise \"or\" operator: |\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt8qiPCl7Lh8"
      },
      "source": [
        "Now we'll filter down the order_products dataframe\n",
        "\n",
        "What conditions could we use for subsetting that table?\n",
        "\n",
        "We can use order_id again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHE_-PKs7e4s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfi9zxpR7ugQ"
      },
      "source": [
        "4) Now we're ready to merge these two tables together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnhC5A2m7yQz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHGqFlPJ80-k"
      },
      "source": [
        "display(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVdIEQsQ8D6x"
      },
      "source": [
        "# Remove columns that we don't need"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLDN1ueb8_pY"
      },
      "source": [
        "Okay, we're looking pretty good, we're missing one more column `product_name` so we're going to need to merge one more time\n",
        "\n",
        "1) merge `orders_and_products` with `products`\n",
        "\n",
        "2) Use `product_id` as our identifier in both tables\n",
        "\n",
        "3) We need to slim down the `products` dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy0fJFKn8--C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yz6FwtG9bhd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd_hwRyC9s1a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL6NrxJLt_iE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaVvDVVltkak"
      },
      "source": [
        "display(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75lbOTf3-csq"
      },
      "source": [
        "### Some nitpicky cleanup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dL8nGeN-eiH"
      },
      "source": [
        "# sort rows\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NODYWri_CYp"
      },
      "source": [
        "# reorder columns\n",
        "final = final[['user_id', 'order_id', 'order_number','order_dow', 'order_hour_of_day', 'add_to_cart_order', 'product_id', 'product_name']]\n",
        "\n",
        "final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSdatcjM-r2B"
      },
      "source": [
        "# remove underscores from column headers\n",
        "\n",
        "# remove underscores from column headers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0blpL6O-99U"
      },
      "source": [
        "display(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNS2FhVW1NxV"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Review this Chis Albon documentation about [concatenating dataframes by row and by column](https://chrisalbon.com/python/data_wrangling/pandas_join_merge_dataframe/) and then be ready to master this function and practice using different `how` parameters on your assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTz5WPngz5BA"
      },
      "source": [
        "# [Objective 3](#tidy) - Learn Tidy Data Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUL61OCE06Dd"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### Why reshape data?\n",
        "\n",
        "#### Some libraries prefer data in different formats\n",
        "\n",
        "For example, the Seaborn data visualization library prefers data in \"Tidy\" format often (but not always).\n",
        "\n",
        "> \"[Seaborn will be most powerful when your datasets have a particular organization.](https://seaborn.pydata.org/introduction.html#organizing-datasets) This format ia alternately called “long-form” or “tidy” data and is described in detail by Hadley Wickham. The rules can be simply stated:\n",
        "\n",
        "> - Each variable is a column\n",
        "- Each observation is a row\n",
        "\n",
        "> A helpful mindset for determining whether your data are tidy is to think backwards from the plot you want to draw. From this perspective, a “variable” is something that will be assigned a role in the plot.\"\n",
        "\n",
        "#### Data science is often about putting square pegs in round holes\n",
        "\n",
        "Here's an inspiring [video clip from _Apollo 13_](https://www.youtube.com/watch?v=ry55--J4_VQ): “Invent a way to put a square peg in a round hole.” It's a good metaphor for data wrangling!"
      ]
    }
  ]
}