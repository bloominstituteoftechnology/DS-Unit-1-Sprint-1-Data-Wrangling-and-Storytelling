{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LS_DS_113_Join_and_Reshape_Data.ipynb","provenance":[{"file_id":"11UhPeYOoq5di-XFhz--92MajLXaz7lu3","timestamp":1601075762909},{"file_id":"1ZkNusAZkoNRMs96GyJau7B9-tUH4oJ27","timestamp":1601007633625},{"file_id":"1YyJKhWIRq8bz9b0d14wxbi_vRN57QvAn","timestamp":1600557306034},{"file_id":"https://github.com/ryanleeallred/DS-Unit-1-Sprint-1-Data-Wrangling-and-Storytelling/blob/master/module3-join-and-reshape-data/LS_DS_113_Join_and_Reshape_Data.ipynb","timestamp":1596643756591}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UQqT5ie4vdDS"},"source":["Lambda School Data Science\n","\n","*Unit 1, Sprint 1, Module 3*\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kOI3YimSvopw"},"source":["# Join and Reshape Data \n","\n","- Objective 01 - concatenate data using the pandas concat method\n","- Objective 02 - merge data using pandas merge\n","- Objective 03 - define the concept of tidy data and describe the format\n","- Objective 04 - transition between tidy and wide data formats with `melt()` and `pivot()`\n","\n","Helpful Links:\n","- [Pandas Cheat Sheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf)\n","- [Tidy Data](https://en.wikipedia.org/wiki/Tidy_data)\n","  - Combine Data Sets: Standard Joins\n","  - Tidy Data\n","  - Reshaping Data\n","- Python Data Science Handbook\n","  - [Chapter 3.6](https://jakevdp.github.io/PythonDataScienceHandbook/03.06-concat-and-append.html), Combining Datasets: Concat and Append\n","  - [Chapter 3.7](https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html), Combining Datasets: Merge and Join\n","  - [Chapter 3.8](https://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html), Aggregation and Grouping\n","  - [Chapter 3.9](https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html), Pivot Tables"]},{"cell_type":"markdown","metadata":{"id":"b_gXHprXvqVx"},"source":["# [Objective 1](#concat) - Concatenate dataframes with pandas\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mPVHZevR04pV"},"source":["## Overview\n","\n","\"Concatenate\" is a fancy word for joining two things together. For example, we can concatenate two strings together using the `+` operator."]},{"cell_type":"code","metadata":{"id":"NeAeYKwN08q3"},"source":["'We can join/concatenate two strings together ' + 'using the \"+\" operator.'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QIgxXomn7iHC"},"source":["When we \"concatenate\" two dataframes we will \"stick them together\" either by rows or columns. Lets look at some simple examples:"]},{"cell_type":"code","metadata":{"id":"O6MbummV9kgH"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1aKHYuH8BTX"},"source":["df1 = pd.DataFrame({'a': [1,2,3,4], 'b': [4,5,6,7], 'c': [7,8,9,10]})\n","\n","df2 = pd.DataFrame({'a': [6,4,8,7], 'b': [9,4,3,2], 'c': [1,6,2,9]})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blLFOpK-8Zwq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"olRWT5VK8bl2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBh-mGzI8k3l"},"source":["### Concatenate by Rows \n","\n","concatenating by rows is the default behavior of `pd.concat()` This is often the most common form of concatenation. "]},{"cell_type":"code","metadata":{"id":"QCw6DJxR8m6T"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iFaja6QacVx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EvZH9k-e8ohe"},"source":["### Concatenate by Columns"]},{"cell_type":"code","metadata":{"id":"-fCzFQxx9D7b"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggv1k1DlbATJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUsQ7O9RbMvu"},"source":["# rename columns\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TCPw9lnbTrY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-NxpMCr9WOS"},"source":["When concatenating dataframes, it is done using the column headers and row index values to match rows up. If these don't match up, then `NaN` values will be added where matches can't be found. "]},{"cell_type":"code","metadata":{"id":"luR-nvD99tBa"},"source":["df3 = pd.DataFrame({'a': [4,3,2,1], 'b': [4,5,6,7], 'c': [7,8,9,10]})\n","\n","df4 = pd.DataFrame({'a': [6,4,8,7,8], 'b': [9,4,3,2,1], 'd': [1,6,2,9,5]})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bj-FdzVf97mn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfZ_wekl99-e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOuoIdey-kCD"},"source":["### Concatenate by rows when not all column headers match"]},{"cell_type":"code","metadata":{"id":"5FpZdgat-EQD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Al203GNp-qVS"},"source":["### Concatenate by columns when not all row indexes match"]},{"cell_type":"code","metadata":{"id":"lc2ngk3O-YCv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUpWwpdSBJGd"},"source":["Whenever we are combining dataframes, if appropriate values cannot be found based on the rules of the method we are using, then missing values will be filled with `NaNs`."]},{"cell_type":"markdown","metadata":{"id":"k8YGJ8Wm1AG3"},"source":["## Follow Along\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hbH8CcozBcyI"},"source":["We’ll work with a dataset of [3 Million Instacart Orders, Open Sourced](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)!\n","\n","The files that we will be working with are in a folder of CSVs, we need to load that folder of CSVs, explore the CSVs to make sure that we understand what we're working with, and where the important data lies, and then work to combine the dataframes together as necessary. \n","\n","\n","\n","Our goal is to reproduce this table which holds the first two orders for user id 1.\n"]},{"cell_type":"code","metadata":{"id":"3xugHGV5C60D"},"source":["from IPython.display import display, Image\n","url = 'https://cdn-images-1.medium.com/max/1600/1*vYGFQCafJtGBBX5mbl0xyw.png'\n","example = Image(url=url, width=600)\n","\n","display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4E3wKrdTChuC"},"source":["#!wget https://s3.amazonaws.com/instacart-datasets/instacart_online_grocery_shopping_2017_05_01.tar.gz \n","\n","# Make sure we're in the top-level /content directory\n","#\n","# See below for notes on the cd command and why it's %cd instead of !cd\n","%cd /content\n","\n","# Remove everything in the current working directory\n","#\n","# rm is the remove command\n","# -rf specifies the \"recursive\" and \"force\" options to remove all files in \n","# subdirectories without prompting\n","#\n","# THIS IS A POWERFUL COMMAND! (NEVER RUN THIS COMMAND ON YOUR COMPUTER)\n","#\n","# In this particular case, removing all of the files makes things easier if you\n","# need to re-run these examples by allowing you start with a clean directory\n","# every time.\n","!rm -rf *\n","\n","# wget retrieves files from a remote location\n","!wget https://www.dropbox.com/s/pofcl26lvoj6073/instacart-market-basket-analysis.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkxqMNGrDJrM"},"source":["# Unzip the archive\n","#\n","# Creates a new directory called instacart-market-basket-analysis\n","\n","!unzip instacart-market-basket-analysis.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaDdXbQqCnGc"},"source":["# Change into the newly-unzipped directory\n","#\n","# % sign is required to change to a new directory -- you can't use !cd like\n","# other commands\n","#\n","# Optional technical details:\n","#\n","# % makes the command apply to the **entire notebook environment**, which is\n","# what you need to do to change the working directory\n","#\n","# The ! sign **opens a new shell process** behind the scenes to execute the\n","# command -- this works fine for regular commands like unzip and ls\n","#\n","# Therefore, !cd would apply only to that new shell and wouldn't change the\n","# global notebook environment\n","#\n","# If this makes your heard hurt, don't worry too much about it. We'll talk\n","# more about the shell and operating systems stuff later in the program.\n","\n","%cd instacart-market-basket-analysis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eADz6civ204P"},"source":["# Unzip all .csv.zip files in the directory\n","!unzip \"*.zip\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nnEkWO4E2_w2"},"source":["# List all csv files in the current directory\n","# -l specifies the \"long\" listing format, which includes additional info on each file\n","# -h specifies \"human readable\" file size units\n","!ls -l -h *.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSV7d0kqfnyJ"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MxwByNLoEG9p"},"source":["### aisles\n","\n","We don't need anything from aisles.csv"]},{"cell_type":"code","metadata":{"id":"pLmGMr_rCoi-"},"source":["aisles = pd.read_csv('aisles.csv')\n","\n","print(aisles.shape)\n","aisles.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTPRZsLvENgJ"},"source":["### departments\n","\n","We don't need anything from departments.csv"]},{"cell_type":"code","metadata":{"id":"fRVjREe8D6yj"},"source":["departments = pd.read_csv('departments.csv')\n","\n","print(departments.shape)\n","departments.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U38aOM6nEWOe"},"source":["### order_products__prior\n","\n","We need:\n","- order id\n","- proudct id\n","- add to cart order\n","\n","Everything except for 'reordered'"]},{"cell_type":"code","metadata":{"id":"-CMWcWSiD8aW"},"source":["order_products__prior = pd.read_csv('order_products__prior.csv')\n","\n","print(order_products__prior.shape)\n","order_products__prior.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KRCdSl5E63N"},"source":["### order_products__train\n","\n","We need:\n","- order id\n","- proudct id\n","- add to cart order\n","\n","Everything except for 'reordered'\n","\n","Do you see anything similar between order_products__train and order_products__prior?\n","\n"]},{"cell_type":"code","metadata":{"id":"2Pq7lgVUD-a-"},"source":["order_products__train = pd.read_csv('order_products__train.csv')\n","\n","print(order_products__train.shape)\n","order_products__train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2cdftjYFj1k"},"source":["### orders\n","\n","We need:\n","- order id\n","- user id\n","- order number\n","- order dow\n","- order hour of day"]},{"cell_type":"code","metadata":{"id":"t9PyqoneEBPd"},"source":["orders = pd.read_csv('orders.csv')\n","\n","print(orders.shape)\n","orders.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yXyRiuIdFmGU"},"source":["### products\n","\n","We need:\n","- product id\n","- product name"]},{"cell_type":"code","metadata":{"id":"3J917C0NEDhG"},"source":["products = pd.read_csv('products.csv')\n","\n","print(products.shape)\n","products.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALhA76X1GkgY"},"source":["## Concatenate order_products__prior and order_products__train\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QU4nkwnPGz4A"},"source":["order_products = pd.concat([order_products__prior, order_products__train])\n","\n","order_products.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrBZ0y8TG09_"},"source":["print(order_products__prior.shape)\n","print(order_products__train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZk8V7yxG2Qg"},"source":["print(order_products.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XSiHrHuj1ME-"},"source":["## Challenge\n","\n","Concatenating dataframes means to stick two dataframes together either by rows or by columns. The default behavior of `pd.concat()` is to take the rows of one dataframe and add them to the rows of another dataframe. If we pass the argument `axis=1` then we will be adding the columns of one dataframe to the columns of another dataframe.\n","\n","Concatenating dataframes is most useful when the columns are the same between two dataframes or when we have matching row indices between two dataframes. \n","\n","Be ready to use this method to combine dataframes together during your assignment."]},{"cell_type":"markdown","metadata":{"id":"17PV3bEtz449"},"source":["# [Objective 2](#merge) - Merge dataframes with pandas\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DAiMlm5Q05LW"},"source":["## Overview"]},{"cell_type":"code","metadata":{"id":"oH4J87G4LZjd"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1o6R29VLwwu"},"source":["Before we can continue we need to understand where the data in the above table is coming from and what why specific pieces of data are held in the specific dataframes.\n","\n","Each of these CSVs has a specific unit of observation (row). The columns that we see included in each CSV were selected purposefully. For example, everything each row of the `orders` dataframe is a specific and unique order -telling us who made the order, and when they made it. Every row in the `products` dataframe tells us about a specific and unique product that thestore offers. And everything in the `order_products` dataframe tells us about how products are associated with specific orders -including when the product was added to the shopping cart. \n","\n","### The Orders Dataframe\n","\n","Holds information about specific orders, things like who placed the order, what \n","\n","- user_id\n","- order_id\n","- order_number\n","- order_dow\n","- order_hour_of_day\n","\n","### The Products Dataframe\n","\n","Holds information about individual products.\n","\n","- product_id\n","- product_name\n","\n","### The Order_Products Dataframe\n","\n","Tells us how products are associated with specific orders since an order is a group of products.\n","\n","- order_id\n","- product_id\n","- add_to_cart_order\n","\n","As we look at the table that we're trying to recreate, we notice that we're not looking at specific orders or products, but at a specific **USER**. We're looking at the first two orders for a specific user and the products associated with those orders, so we'll need to combine dataframes to get all of this data together into a single table.\n","\n","**The key to combining all of this information is that we need values that exist in both datasets that we can use to match up rows and combine dataframes.**"]},{"cell_type":"markdown","metadata":{"id":"g38DqtNj1BnI"},"source":["## Follow Along\n","\n","We have two dataframes, so we're going to need to merge our data twice. As we approach merging datasets together we will take the following approach.\n","\n","1) Identify which to dataframes we would like to combine.\n","\n","2) Find columns that are common between both dataframes that we can use to match up information.\n","\n","3) Slim down both of our dataframes so that they only relevant data before we merge.\n","\n","4) Merge the dataframes."]},{"cell_type":"code","metadata":{"id":"2bY8l5L1msOH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pq1ymXsZnDQJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QH_X78ApndDw"},"source":["## ^^^^^ DON'T DO THIS!\n","\n","I just merged absolutely everything\n","\n"]},{"cell_type":"code","metadata":{"id":"QSuIWJZinrOL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"259BJMJ43Ka5"},"source":["### First Merge\n","\n","1) Combine `orders` and `order_products`\n","\n","2) We will use the `order_id` column to match information between the two datasets\n","\n","3) Lets slim down our dataframes to only the information that we need. We do this because the merge process is complex. Why would we merge millions of rows together if we know that we're only going to need 11 rows when we're done\n","\n","What specific conditions could we use to slim down the `orders` dataframe?\n","\n","`user_id == 1` and `order_id <=2`\n","\n","or\n","\n","`order_id == 2539329` and `order_id == 2398795`"]},{"cell_type":"code","metadata":{"id":"I6r8i8tN1H1S"},"source":["# An example of dataframe filtering\n","\n","# Create a condition\n","\n","\n","# Pass that condition into the square brackets \n","# that we use to access portions of a dataframe\n","# only the rows where that condition evaluates to *TRUE*\n","# will be retained in the dataframe\n","\n","# Look at the subsetted dataframe\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iuvnXjnpPPt"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWUCmgx66Td3"},"source":["# We don't necessarily have to save our condition to the variable \"condition\"\n","# we can pass the condition into the square brackest directly\n","# I just wanted to be clear what was happening inside of the square brackets\n","\n","\n","# orders[0:10,0:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwNuTBbwqeVQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ny-eJnuMqhmj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pga6V2eeqC0m"},"source":["# Filter based on user_id and order_number\n","# AND condition version \n","# I need to use the \"bitwise\" and operator: &\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kQG4sxP6lod"},"source":["Remember there are multiple ways that we could have filtered this dataframe. We also could have done it by specific `order_id`s\n"]},{"cell_type":"code","metadata":{"id":"8IA4Kwyw6vk6"},"source":["# use the bitwise \"or\" operator: |\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nt8qiPCl7Lh8"},"source":["Now we'll filter down the order_products dataframe\n","\n","What conditions could we use for subsetting that table?\n","\n","We can use order_id again."]},{"cell_type":"code","metadata":{"id":"DHE_-PKs7e4s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfi9zxpR7ugQ"},"source":["4) Now we're ready to merge these two tables together."]},{"cell_type":"code","metadata":{"id":"gnhC5A2m7yQz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHGqFlPJ80-k"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVdIEQsQ8D6x"},"source":["# Remove columns that we don't need"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLDN1ueb8_pY"},"source":["Okay, we're looking pretty good, we're missing one more column `product_name` so we're going to need to merge one more time\n","\n","1) merge `orders_and_products` with `products`\n","\n","2) Use `product_id` as our identifier in both tables\n","\n","3) We need to slim down the `products` dataframe"]},{"cell_type":"code","metadata":{"id":"Hy0fJFKn8--C"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7yz6FwtG9bhd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xd_hwRyC9s1a"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TL6NrxJLt_iE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VaVvDVVltkak"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75lbOTf3-csq"},"source":["### Some nitpicky cleanup:"]},{"cell_type":"code","metadata":{"id":"_dL8nGeN-eiH"},"source":["# sort rows\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NODYWri_CYp"},"source":["# reorder columns\n","final = final[['user_id', 'order_id', 'order_number','order_dow', 'order_hour_of_day', 'add_to_cart_order', 'product_id', 'product_name']]\n","\n","final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSdatcjM-r2B"},"source":["# remove underscores from column headers\n","\n","# remove underscores from column headers\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0blpL6O-99U"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sNS2FhVW1NxV"},"source":["## Challenge\n","\n","Review this Chis Albon documentation about [concatenating dataframes by row and by column](https://chrisalbon.com/python/data_wrangling/pandas_join_merge_dataframe/) and then be ready to master this function and practice using different `how` parameters on your assignment."]},{"cell_type":"markdown","metadata":{"id":"xTz5WPngz5BA"},"source":["# [Objective 3](#tidy) - Learn Tidy Data Format"]},{"cell_type":"markdown","metadata":{"id":"rUL61OCE06Dd"},"source":["## Overview\n","\n","### Why reshape data?\n","\n","#### Some libraries prefer data in different formats\n","\n","For example, the Seaborn data visualization library prefers data in \"Tidy\" format often (but not always).\n","\n","> \"[Seaborn will be most powerful when your datasets have a particular organization.](https://seaborn.pydata.org/introduction.html#organizing-datasets) This format ia alternately called “long-form” or “tidy” data and is described in detail by Hadley Wickham. The rules can be simply stated:\n","\n","> - Each variable is a column\n","- Each observation is a row\n","\n","> A helpful mindset for determining whether your data are tidy is to think backwards from the plot you want to draw. From this perspective, a “variable” is something that will be assigned a role in the plot.\"\n","\n","#### Data science is often about putting square pegs in round holes\n","\n","Here's an inspiring [video clip from _Apollo 13_](https://www.youtube.com/watch?v=ry55--J4_VQ): “Invent a way to put a square peg in a round hole.” It's a good metaphor for data wrangling!"]},{"cell_type":"markdown","metadata":{"id":"NoUBeGKlAcCh"},"source":["### Hadley Wickham's Examples\n","\n","From his paper, [Tidy Data](http://vita.had.co.nz/papers/tidy-data.html)"]},{"cell_type":"code","metadata":{"id":"S_b6SOZz091T"},"source":["%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","\n","table1 = pd.DataFrame(\n","    [[np.nan, 2],\n","     [16,    11], \n","     [3,      1]],\n","    index=['John Smith', 'Jane Doe', 'Mary Johnson'], \n","    columns=['treatmenta', 'treatmentb'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gjEInTFNA54j"},"source":["\"Table 1 provides some data about an imaginary experiment in a format commonly seen in the wild. \n","\n","The table has two columns and three rows, and both rows and columns are labelled.\"\n","\n","The data is in a summary table, but we want to have a single record for each observation."]},{"cell_type":"code","metadata":{"id":"fzoZDHtAA30k"},"source":["table1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q7lDPNIwA9t5"},"source":["\"There are many ways to structure the same underlying data. \n","\n","Table 2 shows the same data as Table 1, but the rows and columns have been transposed. The data is the same, but the layout is different.\""]},{"cell_type":"code","metadata":{"id":"kQTFbHJWA_X2"},"source":["# When we swap rows and columns we call that \"transposing\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OfCVyRL3BI5h"},"source":["\"Table 3 reorganiszes Table 1 to make the values, variables and obserations more clear.\n","\n","Table 3 is the tidy version of Table 1. Each row represents an observation, the result of one treatment on one person, and each column is a variable.\"\n","\n","| name         | trt | result |\n","|--------------|-----|--------|\n","| John Smith   | a   | -      |\n","| Jane Doe     | a   | 16     |\n","| Mary Johnson | a   | 3      |\n","| John Smith   | b   | 2      |\n","| Jane Doe     | b   | 11     |\n","| Mary Johnson | b   | 1      |"]},{"cell_type":"markdown","metadata":{"id":"_bZaQTSfBp4s"},"source":["## Follow Along"]},{"cell_type":"markdown","metadata":{"id":"9s42XuCqBN86"},"source":["### Table 1 --> Tidy\n","\n","We can use the pandas `melt` function to reshape Table 1 into Tidy format."]},{"cell_type":"code","metadata":{"id":"cD2bajH1wvU1"},"source":["table1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"91Pw2zgkBHlg"},"source":["# Take the row index, and add it as a new column\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOlvjeBwBTNo"},"source":["# What is the unique identifier for each row\n","# Where is the data at that I want to be in my single \"tidy\" column\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hkXdIKDjBZC4"},"source":["# rename columns\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BnEq18qBbBK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UuFviZYbBebc"},"source":["### Tidy --> Table 1\n","\n","The `pivot_table` function is the inverse of `melt`."]},{"cell_type":"code","metadata":{"id":"LdfbFRI5Bgnh"},"source":["# index: unique identifier\n","# columns: What do you want to differentiate the columns in wide/summary format\n","# values: Where are the numbers at - go in the middle of the summary dataframe\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9H9XAmbTz5D6"},"source":["# [Objective 4](#melt-pivot) - Transition between tidy and summary data formats with `.melt()` and `.pivot()`."]},{"cell_type":"markdown","metadata":{"id":"346ZYlh7vsbx"},"source":["## Overview\n","\n","Tidy data format can be particularly useful with certain plotting libraries like Seaborn for example. Lets practice reshaping our data and show how this can be extremely useful in preparing our data for plotting.\n","\n","Remember that tidy data format means:\n","\n","- Each variable is a column\n","- Each observation is a row\n","\n","A helpful mindset for determining whether your data are tidy is to think backwards from the plot you want to draw. From this perspective, a “variable” is something that will be assigned a role in the plot.\" When plotting, this typically means that the values that we're most interested in and that represent the same thing will all be in a single column. You'll see that in the different examples that we show. The important data will be in a single column.\n","\n"]},{"cell_type":"code","metadata":{"id":"oc3h1LK6ulP-"},"source":["# We'll look more at graphics tomorrow, but we can take a quick look at some of the awesome out-of-the-box seaborn functionality:\n","\n","import seaborn as sns\n","\n","sns.catplot(x='trt', y='result', col='name', kind='bar', data=tidy1, height=4);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDs7HSDtvwp7"},"source":["## Follow Along\n","\n","Now with Instacart Data. We're going to try and reproduce a small part of this visualization: "]},{"cell_type":"code","metadata":{"id":"UYaAh9i2Cth8"},"source":["from IPython.display import display, Image\n","url = 'https://cdn-images-1.medium.com/max/1600/1*wKfV6OV-_1Ipwrl7AjjSuw.png'\n","example = Image(url=url, width=600)\n","\n","display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cWA17E72Csb8"},"source":["Instead of a plot with 50 products, we'll just do two — the first products from each list\n","- Half And Half Ultra Pasteurized\n","- Half Baked Frozen Yogurt\n","\n","So, given a `product_name` we need to calculate its `order_hour_of_day` pattern."]},{"cell_type":"code","metadata":{"id":"Afch5TnbvzYH"},"source":["products = pd.read_csv('products.csv')\n","\n","order_products = pd.concat([pd.read_csv('order_products__prior.csv'), \n","                            pd.read_csv('order_products__train.csv')])\n","\n","orders = pd.read_csv('orders.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-wZZuvJC9XR"},"source":["### Subset and Merge\n","\n","One challenge of performing a merge on this data is that the `products` and `orders` datasets do not have any common columns that we can merge on. Due to this we will have to use the `order_products` dataset to provide the columns that we will use to perform the merge.\n","\n","Here are the two products that we want to work with."]},{"cell_type":"code","metadata":{"id":"8kiwQevWC-ky"},"source":["product_names = ['Half Baked Frozen Yogurt', 'Half And Half Ultra Pasteurized']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQS97tQ4DMcU"},"source":["Lets remind ourselves of what columns we have to work with:"]},{"cell_type":"code","metadata":{"id":"ObLpUAJdDDKs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJSAiUrqDEpP"},"source":["orders.columns.to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bMVJbGMDGeY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_IyvU8jzDJw"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWeDbbc4DSQ9"},"source":["This might blow your mind, but we're going to subset the dataframes to select specific columns **and** merge them all in one go. Ready?"]},{"cell_type":"code","metadata":{"id":"jYG576viDYTY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyCeGkhyDlR0"},"source":["Ok, so we were a little bit lazy and probably should have subsetted our the rows of our dataframes before we merged them. We are going to filter after the fact. This is something that you can try out for practice. Can you figure out how to filter these dataframes **before** merging rather than after?"]},{"cell_type":"code","metadata":{"id":"GRaUjNYQDqlv"},"source":["product_names = ['Half Baked Frozen Yogurt', 'Half And Half Ultra Pasteurized']\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsCbAF1aD6mI"},"source":["Again, there are multiple effective ways to write conditions. "]},{"cell_type":"code","metadata":{"id":"3Q3-vSjFD5r2"},"source":["condition = ((merged['product_name']=='Half Baked Frozen Yogurt') | \n","             (merged['product_name']=='Half And Half Ultra Pasteurized'))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4IxDetIELzM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuGmrZ52ECOS"},"source":["### 4 ways to reshape and plot\n","\n"]},{"cell_type":"code","metadata":{"id":"0FKFamTREFiw"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5udAYCYEQK5"},"source":["1) The `.value_counts()` approach.\n","\n","Remember, that we're trying to get the key variables (values) listed as a single column."]},{"cell_type":"code","metadata":{"id":"mZih8j2QEIb9"},"source":["froyo = subset[subset['product_name']=='Half Baked Frozen Yogurt']\n","cream = subset[subset['product_name']=='Half And Half Ultra Pasteurized']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSjvJq8T0VWc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RVvLVPgeEZXK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QSM7vCH0aKP"},"source":["cream['order_hour_of_day'].value_counts(normalize=True).sort_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxTGM-V60jYs"},"source":["froyo['order_hour_of_day'].value_counts(normalize=True).sort_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jfwc275F0n-t"},"source":["import matplotlib.pyplot as plt\n","\n","(cream['order_hour_of_day'].value_counts(normalize=True).sort_index()\n"," .plot())\n","(froyo['order_hour_of_day'].value_counts(normalize=True).sort_index()\n"," .plot());"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZ7fhirq0tNC"},"source":["display(example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35cWYAYJEdNU"},"source":["2) Crosstab"]},{"cell_type":"code","metadata":{"id":"q5LgBIUSEjCt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPDLGTBe0-CU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtzievA2El7W"},"source":["3) Pivot Table"]},{"cell_type":"code","metadata":{"id":"KfK5LPfQ1OHN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u1zTNxRWEndk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-uexEZoErje"},"source":["4) Melt \n","\n","We've got to get it into wide format first. We'll use a crosstab which is a specific type of pivot_table."]},{"cell_type":"code","metadata":{"id":"l2YFirZPE2DL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGsUDMq8E6TP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blu1emcSE95R"},"source":["Now, with Seaborn:"]},{"cell_type":"code","metadata":{"id":"gTusUe1WE-57"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"weikFLA31q6M"},"source":[""],"execution_count":null,"outputs":[]}]}